{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0f523161",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Word -> Subword\n",
    "def Word_to_sub(word):\n",
    "    \n",
    "    subwords = []\n",
    "    sublist = ['<'] + [w for w in word] + ['>']\n",
    "    \n",
    "    minsub = 2\n",
    "    maxsub = min(6, len(sublist)-1)\n",
    "    \n",
    "    for n_sub in range(minsub, maxsub):\n",
    "        for n in range(len(sublist)-(n_sub-1)):  #subword의 n-gram에서 n에 따라 subword로 나누기\n",
    "            subwords.append(\"\".join(sublist[n:n+n_sub]))\n",
    "    subwords.append(\"\".join(sublist))\n",
    "    return subwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ace6ed9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import kss\n",
    "import re\n",
    "from konlpy.tag import Mecab\n",
    "import numpy as np\n",
    "\n",
    "class Prep():\n",
    "    \n",
    "    def Clean_text(self, text): # noisy한 문장을 cleaning\n",
    "        self.sentences = []\n",
    "        \n",
    "        for i in range(len(text)):\n",
    "            if (i+1) % 100 == 0:\n",
    "                print(str(i+1) + 'th sentence is spliting...')\n",
    "        \n",
    "            temp = re.sub(\"[^가-힣a-z0-9.?]\", ' ', text[i]) #한글, 영어, 숫자, 온점, 물음표가 아닌 것을 공백으로 삭제\n",
    "            temp = re.sub(\"[.]{2,}\", \".\", temp) # 반복되는 온점 (...) 삭제\n",
    "            temp = re.sub(\"[?]{2,}\", \"?\", temp) # 반복되는 물음표 (?) 삭제\n",
    "            temp = re.sub(\"[!]{2,}\", \"!\", temp) # 반복되는 느낌표 (!) 삭제\n",
    "            temp = re.sub(\"[' ']{2,}\", \" \", temp) # 반복되는 공백 삭제 \n",
    "            temp = kss.split_sentences(temp)  #문장 분리\n",
    "\n",
    "            for tmp in temp:\n",
    "                self.sentences.append(tmp)\n",
    "        \n",
    "        return self.sentences\n",
    "        \n",
    "    def Tokenizer(self, sentences): # cleaned data에 대한 형태소 분석 후 vocabulary 구축\n",
    "        \n",
    "        self.corpus = []\n",
    "        self.words = []\n",
    "        self.subs = []\n",
    "        vocab_sub_tmp = []\n",
    "        \n",
    "        tokenizer = Mecab(dicpath=r\"C:\\mecab\\mecab-ko-dic\")\n",
    "        \n",
    "        for sent in sentences:\n",
    "            temp = tokenizer.morphs(sent) # 각 문장별로 형태소 토큰화\n",
    "            self.corpus.append(temp)      # corpus에 형태소 추가\n",
    "            \n",
    "            for tmp in temp:\n",
    "                self.words.append(tmp)    # 각 형태소 모으기\n",
    "                subwords = Word_to_sub(tmp)\n",
    "                self.subs.append(subwords)   #변환된 Subword들 모으기\n",
    "                vocab_sub_tmp += subwords   #각 subword리스트 합치기\n",
    "\n",
    "            self.vocab = set(self.words)    #형태소 vocab\n",
    "            self.vocab_sub = set(vocab_sub_tmp) #Subword vocab\n",
    "        \n",
    "        return self.corpus, self.words, self.subs, self.vocab, self.vocab_sub\n",
    "    \n",
    "    def Make_dict(self,vocab, vocab_sub): # 만들어진 Vocabulary를 기준으로 단어-정수 및 정수-단어 인덱싱 dictionary 생성\n",
    "        self.word_dict = {w: i for i, w in enumerate(vocab)}\n",
    "        self.index_dict = {i: w for i, w in enumerate(vocab)}\n",
    "        \n",
    "        self.subs_dict = {w: i for i, w in enumerate(vocab_sub)}\n",
    "        self.index_subs_dict = {i: w for i, w in enumerate(vocab_sub)}\n",
    "        \n",
    "        return self.word_dict, self.index_dict, self.subs_dict, self.index_subs_dict\n",
    "    \n",
    "    def Get_clean(self, text): #위의 세가지 메소드를 한번에 진행                \n",
    "        sentences =  self.Clean_text(text)\n",
    "        corpus, words, subs, vocab, vocab_sub = self.Tokenizer(sentences)\n",
    "        word_dict, index_dict, subs_dict, subs_index_dict = self.Make_dict(vocab, vocab_sub)\n",
    "        \n",
    "        return sentences, corpus, words, subs, vocab, vocab_sub, word_dict, index_dict, subs_dict, subs_index_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "66842ac8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Make_pairs(): # Input과 target, label을 만들어주는 클래스\n",
    "    \n",
    "    def __init__(self):\n",
    "        \n",
    "        from collections import defaultdict\n",
    "        self.inputs = [] \n",
    "        self.targets = []\n",
    "        self.labels = []\n",
    "        \n",
    "        # Negative sampling 정의\n",
    "        self.wordFreq = defaultdict(int)\n",
    "        for word in words:\n",
    "            self.wordFreq[word] += 1\n",
    "        \n",
    "        self.SumFreq = sum([freq**(3/4) for freq in self.wordFreq.values()]) # 분모\n",
    "        self.wordProb = {word:((freq**(3/4))/self.SumFreq) for word, freq in self.wordFreq.items()} #샘플링 확률분포\n",
    "        \n",
    "    def Draw(self,n_sample): # n_sample : sample을 몇 개를 뽑을건지?\n",
    "        negsam = []\n",
    "        for i in range(n_sample):\n",
    "            negsam.append(np.random.choice(list(self.wordProb.keys()), p=list(self.wordProb.values()))) #확률분포 P에 따른 추출\n",
    "        \n",
    "        return negsam\n",
    "    \n",
    "    def Get_pairs(self, window_size, n_negsam):\n",
    "        for c in corpus: # for each sentence          \n",
    "            for idx in range(len(c)): # for each idx\n",
    "                start = max(0,idx - window_size)\n",
    "                tail = min(idx + window_size, len(c)-1)\n",
    "            \n",
    "                #add negative samples to context\n",
    "                context = c[start:idx] + c[idx+1:tail+1]\n",
    "                needed_negsam = 2*window_size+n_negsam - len(context) # needed number of negative samples\n",
    "                negsam = self.Draw(needed_negsam) # draw negative samples\n",
    "            \n",
    "                #stack pairs\n",
    "                self.inputs.append(Word_to_sub(c[idx]))\n",
    "                self.targets.append(context+negsam)\n",
    "                self.labels.append([1]*len(context) + [0]*needed_negsam)\n",
    "                  \n",
    "        return self.inputs, self.targets, self.labels   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8b5e4037",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.autograd import Variable\n",
    "def train_cuda(inputs, targets, labels, emb_dim, lr, n_epoch):\n",
    "    \n",
    "    vocab_size = len(vocab)\n",
    "    vocab_sub_size = len(vocab_sub)\n",
    "    target_size = len(targets[0])\n",
    "    center_mat = Variable(torch.nn.init.xavier_normal_(torch.empty(emb_dim, vocab_sub_size)).cuda(),requires_grad=True).float() # W mat\n",
    "    context_mat = Variable(torch.nn.init.xavier_normal_(torch.empty(vocab_size, emb_dim)).cuda(),requires_grad=True).float() # W' mat\n",
    "    \n",
    "    for epoch in range(n_epoch): \n",
    "        loss_value = 0\n",
    "        for batch in range(len(inputs)):\n",
    "            \n",
    "            x = inputs[batch] # word index of input\n",
    "            x_sub = Variable(torch.FloatTensor([subs_dict[i] for i in x]).cuda())\n",
    "            h_sub = [center_mat[:,int(i)] for i in x_sub.tolist()] # look-up table\n",
    "            h = sum(h_sub)\n",
    "    \n",
    "            z = torch.matmul(context_mat,h) \n",
    "            y = torch.LongTensor(labels[batch]).cuda()\n",
    "\n",
    "            tar_ind = [word_dict[tar] for tar in targets[batch]]  # make target index                \n",
    "            context_tmp = Variable(context_mat[tar_ind,:], requires_grad=True).float() # embedded target vectors\n",
    "            z = torch.matmul(context_tmp,h) \n",
    "\n",
    "            y_hat = F.log_softmax(z, dim=0)\n",
    "            loss = F.nll_loss(y_hat, y)\n",
    "            loss_value += loss.item()\n",
    "            loss.backward()\n",
    "            \n",
    "            center_mat.data -= lr * center_mat.grad.data # update center_mat\n",
    "            context_mat.data[tar_ind,:] -= lr * context_tmp.grad.data # update context_mat with context_tmp\n",
    "            \n",
    "            # initialize gradient after update\n",
    "            center_mat.grad.data.zero_() \n",
    "            context_tmp.grad.data.zero_()\n",
    "        \n",
    "        if epoch % 100 == 0:\n",
    "            print(f\"Loss at this epoch {epoch+100}: {loss_value / vocab_size}\")\n",
    "        \n",
    "    return center_mat, context_mat         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "53b317c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def TSNEplot(wordvec, vocab, perplexity):\n",
    "    \n",
    "    #matplotlib 패키지 한글 깨짐 처리\n",
    "    import matplotlib.pyplot as plt\n",
    "    %matplotlib notebook\n",
    "    plt.rc('font', family='Malgun Gothic') #윈도우, 구글 콜랩\n",
    "    plt.rcParams['axes.unicode_minus'] = False #한글 폰트 사용시 마이너스 폰트 깨짐 해결\n",
    "\n",
    "    from sklearn.manifold import TSNE\n",
    "    \n",
    "    wordvec_2d = TSNE(n_components=2, perplexity=perplexity, learning_rate='auto', init='random').fit_transform(wordvec)\n",
    "    plt.scatter(wordvec_2d[:,0], wordvec_2d[:,1], s=0.5)\n",
    "\n",
    "    test_words=[]\n",
    "    for i in range(len(vocab)):\n",
    "        test_words.append(index_dict[i])\n",
    "    \n",
    "    for i, word in enumerate(test_words):\n",
    "        plt.annotate(word, xy=(wordvec_2d[i, 0], wordvec_2d[i, 1]))\n",
    "    \n",
    "    return wordvec_2d    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "207e4331",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Korean Sentence Splitter]: Initializing Pynori...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100th sentence is spliting...\n",
      "200th sentence is spliting...\n",
      "300th sentence is spliting...\n",
      "400th sentence is spliting...\n",
      "500th sentence is spliting...\n"
     ]
    }
   ],
   "source": [
    "# Library import\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# 간단한 Corpus\n",
    "f = open(\"C:/Users/ImedisynRnD2/Desktop/KTH/Private/NLP_Study/naver_review_train.txt\", 'rt', encoding='UTF8')\n",
    "line = f.readlines()\n",
    "\n",
    "text = []\n",
    "for l in line:\n",
    "    tmp = l.split('\\t')[1].split('\\t')[0]\n",
    "    text.append(tmp)\n",
    "text = text[1:50000]\n",
    "\n",
    "#parameters\n",
    "emb_dim = 100\n",
    "window_size = 3\n",
    "n_negsam = 15\n",
    "lr = 0.01\n",
    "n_epoch = 300\n",
    "\n",
    "sentences, corpus, words, subs, vocab, vocab_sub, word_dict, index_dict, subs_dict, subs_index_dict = Prep().Get_clean(text) # 전처리\n",
    "inputs, targets, labels = Make_pairs().Get_pairs(window_size, n_negsam) # Input / target / label 생성\n",
    "center_mat, context_mat = train(inputs, targets, labels, emb_dim, lr, n_epoch) # Training\n",
    "# wordvec_2d = TSNEplot(context_mat.data, vocab, perplexity=4) # TNSE plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "8a23e05d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Cos sim v2\n",
    "\n",
    "import numpy as np\n",
    "from numpy import dot\n",
    "from numpy.linalg import norm\n",
    " \n",
    "def cos_sim(word, n=10):\n",
    "    all_sim = []\n",
    "    \n",
    "    subs = Word_to_sub(word) # sub word로 분해\n",
    "    subs = [i for i in subs if i in list(subs_dict.keys())] # sub_dict에 있는 sub들만 취급\n",
    "    \n",
    "    x_sub = [subs_dict[i] for i in subs]\n",
    "    h_sub = [center_mat[:,i] for i in x_sub] # look-up table\n",
    "    h = sum(h_sub).cpu().detach().numpy()\n",
    "    \n",
    "    # vocab의 단어들과 유사도 비교\n",
    "    for v in vocab:\n",
    "        vec = context_mat[word_dict[v],:].cpu().detach().numpy()\n",
    "        sim = dot(h, vec)/(norm(h)*norm(vec))\n",
    "        all_sim.append(sim)\n",
    "    \n",
    "    top = np.argsort(all_sim)[::-1][:n]\n",
    "    outword = [index_dict[i] for i in top]\n",
    "    outsim = [all_sim[t] for t in top]\n",
    "    \n",
    "    print('-------------------------------------')\n",
    "    print('Cosine Similarity of Top %d words' % n)\n",
    "    print('-------------------------------------')\n",
    "    for i in range(n):\n",
    "        print(outword[i] + ': ' + str(outsim[i]))\n",
    "        \n",
    "#     return outword, outsim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ce744988",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'cos_sim' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mC:\\Users\\IMEDIS~1\\AppData\\Local\\Temp/ipykernel_30348/1277830425.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mcos_sim\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'감독'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'cos_sim' is not defined"
     ]
    }
   ],
   "source": [
    "cos_sim('감독')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
