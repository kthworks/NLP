{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1106a65a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ea9b4b11",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Word -> Subword\n",
    "def Word_to_sub(word):\n",
    "    \n",
    "    subwords = []\n",
    "    sublist = ['<'] + [w for w in word] + ['>']\n",
    "    \n",
    "    minsub = 2\n",
    "    maxsub = min(6, len(sublist)-1)\n",
    "    \n",
    "    for n_sub in range(minsub, maxsub):\n",
    "        for n in range(len(sublist)-(n_sub-1)):  #subword의 n-gram에서 n에 따라 subword로 나누기\n",
    "            subwords.append(\"\".join(sublist[n:n+n_sub]))\n",
    "    subwords.append(\"\".join(sublist))\n",
    "    return subwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "af5080f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import kss\n",
    "import re\n",
    "from konlpy.tag import Mecab\n",
    "import numpy as np\n",
    "\n",
    "class Prep():\n",
    "    \n",
    "    def Clean_text(self, text): # noisy한 문장을 cleaning\n",
    "        self.sentences = []\n",
    "        \n",
    "        for i in range(len(text)):\n",
    "            if (i+1) % 100 == 0:\n",
    "                print(str(i+1) + 'th sentence is spliting...')\n",
    "        \n",
    "            temp = re.sub(\"[^가-힣a-z0-9.?]\", ' ', text[i]) #한글, 영어, 숫자, 온점, 물음표가 아닌 것을 공백으로 삭제\n",
    "            temp = re.sub(\"[.]{2,}\", \".\", temp) # 반복되는 온점 (...) 삭제\n",
    "            temp = re.sub(\"[?]{2,}\", \"?\", temp) # 반복되는 물음표 (?) 삭제\n",
    "            temp = re.sub(\"[!]{2,}\", \"!\", temp) # 반복되는 느낌표 (!) 삭제\n",
    "            temp = re.sub(\"[' ']{2,}\", \" \", temp) # 반복되는 공백 삭제 \n",
    "            temp = kss.split_sentences(temp)  #문장 분리\n",
    "\n",
    "            for tmp in temp:\n",
    "                self.sentences.append(tmp)\n",
    "        \n",
    "        return self.sentences\n",
    "        \n",
    "    def Tokenizer(self, sentences): # cleaned data에 대한 형태소 분석 후 vocabulary 구축\n",
    "        \n",
    "        self.corpus = []\n",
    "        self.words = []\n",
    "        self.subs = []\n",
    "        vocab_sub_tmp = []\n",
    "        \n",
    "        tokenizer = Mecab(dicpath=r\"C:\\mecab\\mecab-ko-dic\")\n",
    "        \n",
    "        for sent in sentences:\n",
    "            temp = tokenizer.morphs(sent) # 각 문장별로 형태소 토큰화\n",
    "            self.corpus.append(temp)      # corpus에 형태소 추가\n",
    "            \n",
    "            for tmp in temp:\n",
    "                self.words.append(tmp)    # 각 형태소 모으기\n",
    "                subwords = Word_to_sub(tmp)\n",
    "                self.subs.append(subwords)   #변환된 Subword들 모으기\n",
    "                vocab_sub_tmp += subwords   #각 subword리스트 합치기\n",
    "\n",
    "            self.vocab = set(self.words)    #형태소 vocab\n",
    "            self.vocab_sub = set(vocab_sub_tmp) #Subword vocab\n",
    "        \n",
    "        return self.corpus, self.words, self.subs, self.vocab, self.vocab_sub\n",
    "    \n",
    "    def Make_dict(self,vocab, vocab_sub): # 만들어진 Vocabulary를 기준으로 단어-정수 및 정수-단어 인덱싱 dictionary 생성\n",
    "        self.word_dict = {w: i for i, w in enumerate(vocab)}\n",
    "        self.index_dict = {i: w for i, w in enumerate(vocab)}\n",
    "        \n",
    "        self.subs_dict = {w: i for i, w in enumerate(vocab_sub)}\n",
    "        self.index_subs_dict = {i: w for i, w in enumerate(vocab_sub)}\n",
    "        \n",
    "        return self.word_dict, self.index_dict, self.subs_dict, self.index_subs_dict\n",
    "    \n",
    "    def Get_clean(self, text): #위의 세가지 메소드를 한번에 진행                \n",
    "        sentences =  self.Clean_text(text)\n",
    "        corpus, words, subs, vocab, vocab_sub = self.Tokenizer(sentences)\n",
    "        word_dict, index_dict, subs_dict, subs_index_dict = self.Make_dict(vocab, vocab_sub)\n",
    "        \n",
    "        return sentences, corpus, words, subs, vocab, vocab_sub, word_dict, index_dict, subs_dict, subs_index_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "478dc695",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Make_pairs(): # Input과 target, label을 만들어주는 클래스\n",
    "    \n",
    "    def __init__(self):\n",
    "        \n",
    "        from collections import defaultdict\n",
    "        self.inputs = [] \n",
    "        self.targets = []\n",
    "        self.labels = []\n",
    "        \n",
    "        # Negative sampling 정의\n",
    "        self.wordFreq = defaultdict(int)\n",
    "        for word in words:\n",
    "            self.wordFreq[word] += 1\n",
    "        \n",
    "        self.SumFreq = sum([freq**(3/4) for freq in self.wordFreq.values()]) # 분모\n",
    "        self.wordProb = {word:((freq**(3/4))/self.SumFreq) for word, freq in self.wordFreq.items()} #샘플링 확률분포\n",
    "        \n",
    "    def Draw(self,n_sample): # n_sample : sample을 몇 개를 뽑을건지?\n",
    "        negsam = []\n",
    "        for i in range(n_sample):\n",
    "            negsam.append(np.random.choice(list(self.wordProb.keys()), p=list(self.wordProb.values()))) #확률분포 P에 따른 추출\n",
    "        \n",
    "        return negsam\n",
    "    \n",
    "    def Get_pairs(self, window_size, n_negsam):\n",
    "        for c in corpus: # for each sentence          \n",
    "            for idx in range(len(c)): # for each idx\n",
    "                start = max(0,idx - window_size)\n",
    "                tail = min(idx + window_size, len(c)-1)\n",
    "            \n",
    "                #add negative samples to context\n",
    "                context = c[start:idx] + c[idx+1:tail+1]\n",
    "                needed_negsam = 2*window_size+n_negsam - len(context) # needed number of negative samples\n",
    "                negsam = self.Draw(needed_negsam) # draw negative samples\n",
    "            \n",
    "                #stack pairs\n",
    "                self.inputs.append(Word_to_sub(c[idx]))\n",
    "                self.targets.append(context+negsam)\n",
    "                self.labels.append([1]*len(context) + [0]*needed_negsam)\n",
    "                  \n",
    "        return self.inputs, self.targets, self.labels   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a6b41c51",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.autograd import Variable\n",
    "def train(inputs, targets, labels, emb_dim, lr, n_epoch):\n",
    "    \n",
    "    vocab_size = len(vocab)\n",
    "    vocab_sub_size = len(vocab_sub)\n",
    "    target_size = len(targets[0])\n",
    "    center_mat = Variable(torch.nn.init.xavier_normal_(torch.empty(emb_dim, vocab_sub_size)).cuda(),requires_grad=True).float() # W mat\n",
    "    context_mat = Variable(torch.nn.init.xavier_normal_(torch.empty(vocab_size, emb_dim)).cuda(),requires_grad=True).float() # W' mat\n",
    "    \n",
    "    for epoch in range(n_epoch): \n",
    "        loss_value = 0\n",
    "        for batch in range(len(inputs)):\n",
    "            \n",
    "            x = inputs[batch] # word index of input\n",
    "            x_sub = Variable(torch.FloatTensor([subs_dict[i] for i in x]).cuda())\n",
    "            h_sub = [center_mat[:,int(i)] for i in x_sub.tolist()] # look-up table\n",
    "            h = sum(h_sub)\n",
    "    \n",
    "            z = torch.matmul(context_mat,h) \n",
    "            y = torch.LongTensor(labels[batch]).cuda()\n",
    "\n",
    "            tar_ind = [word_dict[tar] for tar in targets[batch]]  # make target index                \n",
    "            context_tmp = Variable(context_mat[tar_ind,:], requires_grad=True).float() # embedded target vectors\n",
    "            z = torch.matmul(context_tmp,h) \n",
    "\n",
    "            y_hat = F.log_softmax(z, dim=0)\n",
    "            loss = F.nll_loss(y_hat, y)\n",
    "            loss_value += loss.item()\n",
    "            loss.backward()\n",
    "            \n",
    "            center_mat.data -= lr * center_mat.grad.data # update center_mat\n",
    "            context_mat.data[tar_ind,:] -= lr * context_tmp.grad.data # update context_mat with context_tmp\n",
    "            \n",
    "            # initialize gradient after update\n",
    "            center_mat.grad.data.zero_() \n",
    "            context_tmp.grad.data.zero_()\n",
    "        \n",
    "        if epoch % 100 == 0:\n",
    "            print(f\"Loss at this epoch {epoch+100}: {loss_value / vocab_size}\")\n",
    "        \n",
    "    return center_mat, context_mat         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b1d47a7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def TSNEplot(wordvec, vocab, perplexity):\n",
    "    \n",
    "    #matplotlib 패키지 한글 깨짐 처리\n",
    "    import matplotlib.pyplot as plt\n",
    "    %matplotlib notebook\n",
    "    plt.rc('font', family='Malgun Gothic') #윈도우, 구글 콜랩\n",
    "    plt.rcParams['axes.unicode_minus'] = False #한글 폰트 사용시 마이너스 폰트 깨짐 해결\n",
    "\n",
    "    from sklearn.manifold import TSNE\n",
    "    \n",
    "    wordvec_2d = TSNE(n_components=2, perplexity=perplexity, learning_rate='auto', init='random').fit_transform(wordvec)\n",
    "    plt.scatter(wordvec_2d[:,0], wordvec_2d[:,1], s=0.5)\n",
    "\n",
    "    test_words=[]\n",
    "    for i in range(len(vocab)):\n",
    "        test_words.append(index_dict[i])\n",
    "    \n",
    "    for i, word in enumerate(test_words):\n",
    "        plt.annotate(word, xy=(wordvec_2d[i, 0], wordvec_2d[i, 1]))\n",
    "    \n",
    "    return wordvec_2d    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85aa34ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100th sentence is spliting...\n",
      "200th sentence is spliting...\n",
      "300th sentence is spliting...\n",
      "400th sentence is spliting...\n",
      "500th sentence is spliting...\n",
      "600th sentence is spliting...\n",
      "700th sentence is spliting...\n",
      "800th sentence is spliting...\n",
      "900th sentence is spliting...\n",
      "1000th sentence is spliting...\n",
      "1100th sentence is spliting...\n",
      "1200th sentence is spliting...\n",
      "1300th sentence is spliting...\n",
      "1400th sentence is spliting...\n",
      "1500th sentence is spliting...\n",
      "1600th sentence is spliting...\n",
      "1700th sentence is spliting...\n",
      "1800th sentence is spliting...\n",
      "1900th sentence is spliting...\n",
      "2000th sentence is spliting...\n",
      "2100th sentence is spliting...\n",
      "2200th sentence is spliting...\n",
      "2300th sentence is spliting...\n",
      "2400th sentence is spliting...\n",
      "2500th sentence is spliting...\n",
      "2600th sentence is spliting...\n",
      "2700th sentence is spliting...\n",
      "2800th sentence is spliting...\n",
      "2900th sentence is spliting...\n",
      "3000th sentence is spliting...\n",
      "3100th sentence is spliting...\n",
      "3200th sentence is spliting...\n",
      "3300th sentence is spliting...\n",
      "3400th sentence is spliting...\n",
      "3500th sentence is spliting...\n",
      "3600th sentence is spliting...\n",
      "3700th sentence is spliting...\n",
      "3800th sentence is spliting...\n",
      "3900th sentence is spliting...\n",
      "4000th sentence is spliting...\n",
      "4100th sentence is spliting...\n",
      "4200th sentence is spliting...\n",
      "4300th sentence is spliting...\n",
      "4400th sentence is spliting...\n",
      "4500th sentence is spliting...\n",
      "4600th sentence is spliting...\n",
      "4700th sentence is spliting...\n",
      "4800th sentence is spliting...\n",
      "4900th sentence is spliting...\n",
      "Loss at this epoch 100: 24.60635722289041\n"
     ]
    }
   ],
   "source": [
    "# Library import\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "# 간단한 Corpus\n",
    "f = open(\"C:/Users/ImedisynRnD2/Desktop/KTH/Private/NLP_Study/naver_review_train.txt\", 'rt', encoding='UTF8')\n",
    "line = f.readlines()\n",
    "\n",
    "text = []\n",
    "for l in line:\n",
    "    tmp = l.split('\\t')[1].split('\\t')[0]\n",
    "    text.append(tmp)\n",
    "text = text[1:5000]\n",
    "\n",
    "#parameters\n",
    "emb_dim = 20\n",
    "window_size = 2\n",
    "n_negsam = 10\n",
    "lr = 0.01\n",
    "n_epoch = 300\n",
    "\n",
    "sentences, corpus, words, subs, vocab, vocab_sub, word_dict, index_dict, subs_dict, subs_index_dict = Prep().Get_clean(text) # 전처리\n",
    "inputs, targets, labels = Make_pairs().Get_pairs(window_size, n_negsam) # Input / target / label 생성\n",
    "center_mat, context_mat = train(inputs, targets, labels, emb_dim, lr, n_epoch) # Training\n",
    "# wordvec_2d = TSNEplot(context_mat.data, vocab, perplexity=4) # TNSE plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b7789491",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Cos sim v2\n",
    "\n",
    "import numpy as np\n",
    "from numpy import dot\n",
    "from numpy.linalg import norm\n",
    " \n",
    "def cos_sim(word, n=10):\n",
    "    all_sim = []\n",
    "    \n",
    "    subs = Word_to_sub(word) # sub word로 분해\n",
    "    subs = [i for i in subs if i in list(subs_dict.keys())] # sub_dict에 있는 sub들만 취급\n",
    "    \n",
    "    x_sub = [subs_dict[i] for i in subs]\n",
    "    h_sub = [center_mat[:,i] for i in x_sub] # look-up table\n",
    "    h = sum(h_sub).cpu().detach().numpy()\n",
    "    \n",
    "    # vocab의 단어들과 유사도 비교\n",
    "    for v in vocab:\n",
    "        vec = context_mat[word_dict[v],:].cpu().detach().numpy()\n",
    "        sim = dot(h, vec)/(norm(h)*norm(vec))\n",
    "        all_sim.append(sim)\n",
    "    \n",
    "    top = np.argsort(all_sim)[::-1][:n]\n",
    "    outword = [index_dict[i] for i in top]\n",
    "    outsim = [all_sim[t] for t in top]\n",
    "    \n",
    "    print('-------------------------------------')\n",
    "    print('Cosine Similarity of Top %d words' % n)\n",
    "    print('-------------------------------------')\n",
    "    for i in range(n):\n",
    "        print(outword[i] + ': ' + str(outsim[i]))\n",
    "        \n",
    "#     return outword, outsim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "3666c6c3",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------\n",
      "Cosine Similarity of Top 10 words\n",
      "-------------------------------------\n",
      "고역: 0.77677375\n",
      "위원: 0.7241702\n",
      "유대인: 0.7110779\n",
      "태어났: 0.654795\n",
      "눈부신: 0.6399819\n",
      "과하: 0.62564206\n",
      "사슴: 0.617252\n",
      "발연기: 0.614777\n",
      "빼: 0.6125042\n",
      "고추: 0.6087969\n"
     ]
    }
   ],
   "source": [
    "cos_sim('인공지능채팅')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
