{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "ec7b3534",
   "metadata": {},
   "outputs": [],
   "source": [
    "import kss\n",
    "import re\n",
    "from konlpy.tag import Mecab\n",
    "\n",
    "class Prep():\n",
    "        \n",
    "    def Clean_text(self, text): # noisy한 문장을 cleaning\n",
    "        self.sentences = []\n",
    "        \n",
    "        for i in range(len(text)):\n",
    "            if (i+1) % 10 == 0:\n",
    "                print(str(i+1) + 'th sentence is spliting...')\n",
    "        \n",
    "            temp = re.sub(\"[^가-힣a-z0-9.?]\", ' ', text[i]) #한글, 영어, 숫자, 온점, 물음표가 아닌 것을 공백으로 삭제\n",
    "            temp = re.sub(\"[.]{2,}\", \".\", temp) # 반복되는 온점 (...) 삭제\n",
    "            temp = re.sub(\"[?]{2,}\", \"?\", temp) # 반복되는 물음표 (?) 삭제\n",
    "            temp = re.sub(\"[!]{2,}\", \"!\", temp) # 반복되는 느낌표 (!) 삭제\n",
    "            temp = re.sub(\"[' ']{2,}\", \" \", temp) # 반복되는 공백 삭제 \n",
    "            temp = kss.split_sentences(temp)  #문장 분리\n",
    "\n",
    "            for tmp in temp:\n",
    "                self.sentences.append(tmp)\n",
    "        \n",
    "        return self.sentences\n",
    "    \n",
    "    def Tokenizer(self, sentences, n_sub): # cleaned data에 대한 형태소 분석 후 vocabulary 구축\n",
    "        \n",
    "        self.corpus = []\n",
    "        self.words = []\n",
    "        self.subs = []\n",
    "        vocab_sub_tmp = []\n",
    "        \n",
    "        tokenizer = Mecab(dicpath=r\"C:\\mecab\\mecab-ko-dic\")\n",
    "        \n",
    "        for sent in sentences:\n",
    "            temp = tokenizer.morphs(sent) # 각 문장별로 형태소 토큰화\n",
    "            self.corpus.append(temp)      # corpus에 형태소 추가\n",
    "            \n",
    "            for tmp in temp:\n",
    "                self.words.append(tmp)    # 각 형태소 모으기\n",
    "        \n",
    "                ### Word -> Subword\n",
    "                subword_tmp = []\n",
    "                sublist = ['<'] + [t for t in tmp] + ['>']\n",
    "\n",
    "                for n in range(len(sublist)-(n_sub-1)):  #subword의 n-gram에서 n에 따라 subword로 나누기\n",
    "                    subword_tmp.append(\"\".join(sublist[n:n+n_sub]))\n",
    "\n",
    "                self.subs.append(subword_tmp)   #변환된 Subword들 모으기\n",
    "                vocab_sub_tmp += subword_tmp   #각 subword리스트 합치기\n",
    "\n",
    "            self.vocab = set(self.words)    #형태소 vocab\n",
    "            self.vocab_sub = set(vocab_sub_tmp) #Subword vocab\n",
    "        \n",
    "        return self.corpus, self.words, self.subs, self.vocab, self.vocab_sub\n",
    "    \n",
    "    def Make_dict(self,vocab): # 만들어진 Vocabulary를 기준으로 단어-정수 및 정수-단어 인덱싱 dictionary 생성\n",
    "        self.word_dict = {w: i for i, w in enumerate(vocab)}\n",
    "        self.index_dict = {i: w for i, w in enumerate(vocab)}\n",
    "        return self.word_dict, self.index_dict \n",
    "    \n",
    "    def Get_clean(self, text): #위의 세가지 메소드를 한번에 진행                \n",
    "        sentences =  self.Clean_text(text)\n",
    "        corpus, words, vocab = self.Tokenizer(sentences)\n",
    "        word_dict, index_dict = self.Make_dict(vocab)\n",
    "        \n",
    "        return sentences, corpus, words, vocab, word_dict, index_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7dcede40",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Make_pairs(): # Input과 target, label을 만들어주는 클래스\n",
    "    \n",
    "    def __init__(self):\n",
    "        \n",
    "        from collections import defaultdict\n",
    "        self.inputs = [] \n",
    "        self.targets = []\n",
    "        self.labels = []\n",
    "        \n",
    "        # Negative sampling 정의\n",
    "        self.wordFreq = defaultdict(int)\n",
    "        for word in words:\n",
    "            self.wordFreq[word] += 1\n",
    "        \n",
    "        self.SumFreq = sum([freq**(3/4) for freq in self.wordFreq.values()]) # 분모\n",
    "        self.wordProb = {word:((freq**(3/4))/self.SumFreq) for word, freq in self.wordFreq.items()} #샘플링 확률분포\n",
    "        \n",
    "    def Draw(self,n_sample): # n_sample : sample을 몇 개를 뽑을건지?\n",
    "        negsam = []\n",
    "        for i in range(n_sample):\n",
    "            negsam.append(np.random.choice(list(self.wordProb.keys()), p=list(self.wordProb.values()))) #확률분포 P에 따른 추출\n",
    "        \n",
    "        return negsam\n",
    "    \n",
    "    def Get_pairs(self, window_size, n_negsam):\n",
    "        for c in corpus: # for each sentence          \n",
    "            for idx in range(len(c)): # for each idx\n",
    "                start = max(0,idx - window_size)\n",
    "                tail = min(idx + window_size, len(c)-1)\n",
    "            \n",
    "                #add negative samples to context\n",
    "                context = c[start:idx] + c[idx+1:tail+1]\n",
    "                needed_negsam = 2*window_size+n_negsam - len(context) # needed number of negative samples\n",
    "                negsam = self.Draw(needed_negsam) # draw negative samples\n",
    "            \n",
    "                #stack pairs\n",
    "                self.inputs.append(c[idx])\n",
    "                self.targets.append(context+negsam)\n",
    "                self.labels.append([1]*len(context) + [0]*needed_negsam)\n",
    "                  \n",
    "        return self.inputs, self.targets, self.labels   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a41581e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.autograd import Variable\n",
    "def train(inputs, targets, labels, emb_dim, lr, n_epoch):\n",
    "    \n",
    "    vocab_size = len(vocab)\n",
    "    target_size = len(targets[0])\n",
    "    center_mat = Variable(torch.nn.init.xavier_normal_(torch.empty(emb_dim, vocab_size)),requires_grad=True).float() # W mat\n",
    "    context_mat = Variable(torch.nn.init.xavier_normal_(torch.empty(vocab_size, emb_dim)),requires_grad=True).float() # W' mat\n",
    "    \n",
    "    for epoch in range(n_epoch): \n",
    "        loss_value = 0\n",
    "        for batch in range(len(inputs)): \n",
    "            x = word_dict[inputs[batch]] # word index of input\n",
    "            h = center_mat[:,x] # look-up table\n",
    "            \n",
    "            tar_ind = [word_dict[tar] for tar in targets[batch]]  # make target index\n",
    "                \n",
    "            context_tmp = Variable(context_mat[tar_ind,:], requires_grad=True).float() # embedded target vectors\n",
    "            z = torch.matmul(context_tmp,h) \n",
    "            y = torch.LongTensor(labels[batch])\n",
    "\n",
    "            y_hat = F.log_softmax(z, dim=0)\n",
    "            loss = F.nll_loss(y_hat, y)\n",
    "            loss_value += loss.item()\n",
    "            loss.backward()\n",
    "            \n",
    "            center_mat.data -= lr * center_mat.grad.data # update center_mat\n",
    "            context_mat.data[tar_ind,:] -= lr * context_tmp.grad.data # update context_mat with context_tmp\n",
    "            \n",
    "            # initialize gradient after update\n",
    "            center_mat.grad.data.zero_() \n",
    "            context_tmp.grad.data.zero_()\n",
    "        \n",
    "        if epoch % 100 == 0:\n",
    "            print(f\"Loss at this epoch {epoch+100}: {loss_value / vocab_size}\")\n",
    "        \n",
    "    return center_mat         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dd562c0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Korean Sentence Splitter]: Initializing Pynori...\n"
     ]
    }
   ],
   "source": [
    "# Library import\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# 간단한 Corpus\n",
    "text = ['대한민국의 수도는 서울',\n",
    "       '독일의 수도는 베를린',\n",
    "       '프랑스의 수도는 파리',\n",
    "       '스위스의 수도는 베른',\n",
    "       '일본의 수도는 도쿄',\n",
    "       '이탈리아의 수도는 로마',\n",
    "       '영국의 수도는 런던',\n",
    "       '이집트의 수도는 카이로'\n",
    "       ]        \n",
    "\n",
    "#parameters\n",
    "emb_dim = 20\n",
    "window_size = 2\n",
    "n_negsam = 3\n",
    "lr = 0.01\n",
    "n_epoch = 1000\n",
    "\n",
    "sentences, corpus, words, vocab, word_dict, index_dict = Prep().Get_clean(text) # 전처리\n",
    "inputs, targets, labels = Make_pairs().Get_pairs(window_size, n_negsam) # Input / target / label 생성\n",
    "# center_mat = train(inputs, targets, labels, emb_dim, lr, n_epoch) # Training\n",
    "# wordvec_2d = TSNEplot(center_mat.data.T, vocab, perplexity=4) # TNSE plotting"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
