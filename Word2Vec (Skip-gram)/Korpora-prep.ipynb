{
 "cells": [
  {
   "cell_type": "raw",
   "id": "c8820b13",
   "metadata": {},
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55bbb3dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from Korpora import Korpora\n",
    "corpus = Korpora.load(\"korean_petitions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "466ae382",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_text = corpus.train.texts\n",
    "\n",
    "text = []\n",
    "for i in range(10):              # 게시글 10개만 추출\n",
    "    text.append(all_text[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41401b96",
   "metadata": {},
   "outputs": [],
   "source": [
    "import kss\n",
    "import re\n",
    "from konlpy.tag import Mecab\n",
    "\n",
    "class Prep():\n",
    "        \n",
    "    def Clean_text(self, text):\n",
    "\n",
    "        self.sentences = []\n",
    "        \n",
    "        for i in range(len(text)):\n",
    "            if (i+1) % 10 == 0:\n",
    "                print(str(i+1) + 'th sentence is spliting...')\n",
    "        \n",
    "            temp = re.sub(\"[^가-힣a-z0-9.?]\", ' ', text[i]) #한글, 영어, 숫자, 온점, 물음표가 아닌 것을 공백으로 삭제\n",
    "            temp = re.sub(\"[.]{2,}\", \".\", temp) # 반복되는 온점 (...) 삭제\n",
    "            temp = re.sub(\"[?]{2,}\", \"?\", temp) # 반복되는 물음표 (?) 삭제\n",
    "            temp = re.sub(\"[!]{2,}\", \"!\", temp) # 반복되는 느낌표 (!) 삭제\n",
    "            temp = re.sub(\"[' ']{2,}\", \" \", temp) # 반복되는 공백 삭제 \n",
    "            temp = kss.split_sentences(temp)  #문장 분리\n",
    "\n",
    "            for tmp in temp:\n",
    "                self.sentences.append(tmp)\n",
    "        \n",
    "        return self.sentences\n",
    "\n",
    "        \n",
    "    def Tokenizer(self, sentences):\n",
    "        \n",
    "        self.corpus = []\n",
    "        self.words = []\n",
    "        self.vocab = []\n",
    "        \n",
    "        tokenizer = Mecab(dicpath=r\"C:\\mecab\\mecab-ko-dic\")\n",
    "        \n",
    "        for sent in self.sentences:\n",
    "            temp = tokenizer.morphs(sent)\n",
    "            self.corpus.append(temp)\n",
    "            \n",
    "            for tmp in temp:\n",
    "                self.words.append(tmp)\n",
    "\n",
    "        self.vocab = set(self.words)\n",
    "        \n",
    "        return self.corpus, self.words, self.vocab\n",
    "    \n",
    "    def Get_clean(self, text):\n",
    "        sentences =  prep.Clean_text(text)\n",
    "        corpus, words, vocab = prep.Tokenizer(self.sentences)\n",
    "        print('Done!')\n",
    "        \n",
    "        return sentences, corpus, words, vocab\n",
    "        \n",
    "\n",
    "prep = Prep()\n",
    "sentences, corpus, words, vocab = prep.Get_clean(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d02aeff9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('# of sentences : ' , len(sentences)) # 전체 문장 수 확인\n",
    "print('# of corpus : ', len(corpus)) # 전체 corpus 문장 수 확인 (corpus는 각 문장을 형태소단위로 분리해놓은 것)\n",
    "print('# of all words : ' , len(words)) # 전체 단어 수 확인(단어 중복 포함)\n",
    "print('# of words in vocabulary : ' , len(vocab)) # vocabulary 확인 (단어 중복 제거) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CUDA",
   "language": "python",
   "name": "cuda"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
