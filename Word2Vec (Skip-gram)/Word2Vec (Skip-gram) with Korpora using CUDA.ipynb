{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "659c8522",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "189e2056",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    Korpora 는 다른 분들이 연구 목적으로 공유해주신 말뭉치들을\n",
      "    손쉽게 다운로드, 사용할 수 있는 기능만을 제공합니다.\n",
      "\n",
      "    말뭉치들을 공유해 주신 분들에게 감사드리며, 각 말뭉치 별 설명과 라이센스를 공유 드립니다.\n",
      "    해당 말뭉치에 대해 자세히 알고 싶으신 분은 아래의 description 을 참고,\n",
      "    해당 말뭉치를 연구/상용의 목적으로 이용하실 때에는 아래의 라이센스를 참고해 주시기 바랍니다.\n",
      "\n",
      "    # Description\n",
      "    Author : Hyunjoong Kim lovit@github\n",
      "    Repository : https://github.com/lovit/petitions_archive\n",
      "    References :\n",
      "\n",
      "    청와대 국민청원 게시판의 데이터를 월별로 수집한 것입니다.\n",
      "    청원은 게시판에 글을 올린 뒤, 한달 간 청원이 진행됩니다.\n",
      "    수집되는 데이터는 청원종료가 된 이후의 데이터이며, 청원 내 댓글은 수집되지 않습니다.\n",
      "    단 청원의 동의 개수는 수집됩니다.\n",
      "    자세한 내용은 위의 repository를 참고하세요.\n",
      "\n",
      "    # License\n",
      "    CC0 1.0 Universal (CC0 1.0) Public Domain Dedication\n",
      "    Details in https://creativecommons.org/publicdomain/zero/1.0/\n",
      "\n",
      "[Korpora] Corpus `korean_petitions` is already installed at C:\\Users\\ImedisynRnD2\\Korpora\\korean_petitions\\petitions_2017-08\n",
      "[Korpora] Corpus `korean_petitions` is already installed at C:\\Users\\ImedisynRnD2\\Korpora\\korean_petitions\\petitions_2017-09\n",
      "[Korpora] Corpus `korean_petitions` is already installed at C:\\Users\\ImedisynRnD2\\Korpora\\korean_petitions\\petitions_2017-10\n",
      "[Korpora] Corpus `korean_petitions` is already installed at C:\\Users\\ImedisynRnD2\\Korpora\\korean_petitions\\petitions_2017-11\n",
      "[Korpora] Corpus `korean_petitions` is already installed at C:\\Users\\ImedisynRnD2\\Korpora\\korean_petitions\\petitions_2017-12\n",
      "[Korpora] Corpus `korean_petitions` is already installed at C:\\Users\\ImedisynRnD2\\Korpora\\korean_petitions\\petitions_2018-01\n",
      "[Korpora] Corpus `korean_petitions` is already installed at C:\\Users\\ImedisynRnD2\\Korpora\\korean_petitions\\petitions_2018-02\n",
      "[Korpora] Corpus `korean_petitions` is already installed at C:\\Users\\ImedisynRnD2\\Korpora\\korean_petitions\\petitions_2018-03\n",
      "[Korpora] Corpus `korean_petitions` is already installed at C:\\Users\\ImedisynRnD2\\Korpora\\korean_petitions\\petitions_2018-04\n",
      "[Korpora] Corpus `korean_petitions` is already installed at C:\\Users\\ImedisynRnD2\\Korpora\\korean_petitions\\petitions_2018-05\n",
      "[Korpora] Corpus `korean_petitions` is already installed at C:\\Users\\ImedisynRnD2\\Korpora\\korean_petitions\\petitions_2018-06\n",
      "[Korpora] Corpus `korean_petitions` is already installed at C:\\Users\\ImedisynRnD2\\Korpora\\korean_petitions\\petitions_2018-07\n",
      "[Korpora] Corpus `korean_petitions` is already installed at C:\\Users\\ImedisynRnD2\\Korpora\\korean_petitions\\petitions_2018-08\n",
      "[Korpora] Corpus `korean_petitions` is already installed at C:\\Users\\ImedisynRnD2\\Korpora\\korean_petitions\\petitions_2018-09\n",
      "[Korpora] Corpus `korean_petitions` is already installed at C:\\Users\\ImedisynRnD2\\Korpora\\korean_petitions\\petitions_2018-10\n",
      "[Korpora] Corpus `korean_petitions` is already installed at C:\\Users\\ImedisynRnD2\\Korpora\\korean_petitions\\petitions_2018-11\n",
      "[Korpora] Corpus `korean_petitions` is already installed at C:\\Users\\ImedisynRnD2\\Korpora\\korean_petitions\\petitions_2018-12\n",
      "[Korpora] Corpus `korean_petitions` is already installed at C:\\Users\\ImedisynRnD2\\Korpora\\korean_petitions\\petitions_2019-01\n",
      "[Korpora] Corpus `korean_petitions` is already installed at C:\\Users\\ImedisynRnD2\\Korpora\\korean_petitions\\petitions_2019-02\n",
      "[Korpora] Corpus `korean_petitions` is already installed at C:\\Users\\ImedisynRnD2\\Korpora\\korean_petitions\\petitions_2019-03\n"
     ]
    }
   ],
   "source": [
    "from Korpora import Korpora\n",
    "Corpus = Korpora.load(\"korean_petitions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "04c74614",
   "metadata": {},
   "outputs": [],
   "source": [
    "import kss\n",
    "import re\n",
    "from konlpy.tag import Mecab\n",
    "\n",
    "class Prep():\n",
    "        \n",
    "    def Clean_text(self, text): # noisy한 문장을 cleaning\n",
    "        self.sentences = []\n",
    "        \n",
    "        for i in range(len(text)):\n",
    "            if (i+1) % 10 == 0:\n",
    "                print(str(i+1) + 'th sentence is spliting...')\n",
    "        \n",
    "            temp = re.sub(\"[^가-힣a-z0-9.?]\", ' ', text[i]) #한글, 영어, 숫자, 온점, 물음표가 아닌 것을 공백으로 삭제\n",
    "            temp = re.sub(\"[.]{2,}\", \".\", temp) # 반복되는 온점 (...) 삭제\n",
    "            temp = re.sub(\"[?]{2,}\", \"?\", temp) # 반복되는 물음표 (?) 삭제\n",
    "            temp = re.sub(\"[!]{2,}\", \"!\", temp) # 반복되는 느낌표 (!) 삭제\n",
    "            temp = re.sub(\"[' ']{2,}\", \" \", temp) # 반복되는 공백 삭제 \n",
    "            temp = kss.split_sentences(temp)  #문장 분리\n",
    "\n",
    "            for tmp in temp:\n",
    "                self.sentences.append(tmp)\n",
    "        \n",
    "        return self.sentences\n",
    "\n",
    "        \n",
    "    def Tokenizer(self, sentences): # cleaned data에 대한 형태소 분석 후 vocabulary 구축\n",
    "        \n",
    "        self.corpus = []\n",
    "        self.words = []\n",
    "        self.vocab = []\n",
    "        \n",
    "        tokenizer = Mecab(dicpath=r\"C:\\mecab\\mecab-ko-dic\")\n",
    "        \n",
    "        for sent in self.sentences:\n",
    "            temp = tokenizer.morphs(sent)\n",
    "            self.corpus.append(temp)\n",
    "            \n",
    "            for tmp in temp:\n",
    "                self.words.append(tmp)\n",
    "\n",
    "        self.vocab = set(self.words)\n",
    "        \n",
    "        return self.corpus, self.words, self.vocab\n",
    "    \n",
    "    def Make_dict(self,vocab): # 만들어진 Vocabulary를 기준으로 단어-정수 및 정수-단어 인덱싱 dictionary 생성\n",
    "        self.word_dict = {w: i for i, w in enumerate(vocab)}\n",
    "        self.index_dict = {i: w for i, w in enumerate(vocab)}\n",
    "        return self.word_dict, self.index_dict \n",
    "    \n",
    "    def Get_clean(self, text): #위의 세가지 메소드를 한번에 진행                \n",
    "        sentences =  self.Clean_text(text)\n",
    "        corpus, words, vocab = self.Tokenizer(sentences)\n",
    "        word_dict, index_dict = self.Make_dict(vocab)\n",
    "        \n",
    "        return sentences, corpus, words, vocab, word_dict, index_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "id": "1df00cce",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Make_pairs(): # Input과 target, label을 만들어주는 클래스\n",
    "    \n",
    "    def __init__(self):\n",
    "        \n",
    "        from collections import defaultdict\n",
    "        self.inputs = [] \n",
    "        self.targets = []\n",
    "        self.labels = []\n",
    "        self.vocab_size = len(vocab)\n",
    "        \n",
    "        # Negative sampling 정의\n",
    "        self.wordFreq = defaultdict(int)\n",
    "        for word in words:\n",
    "            self.wordFreq[word] += 1\n",
    "        \n",
    "        self.SumFreq = sum([freq**(3/4) for freq in self.wordFreq.values()]) # 분모\n",
    "        self.wordProb = {word:((freq**(3/4))/self.SumFreq) for word, freq in self.wordFreq.items()} #샘플링 확률분포\n",
    "        \n",
    "    def Draw(self,n_sample): # n_sample : sample을 몇 개를 뽑을건지?\n",
    "        negsam = []\n",
    "        for i in range(n_sample):\n",
    "            negsam.append(np.random.choice(list(self.wordProb.keys()), p=list(self.wordProb.values()))) #확률분포 P에 따른 추출\n",
    "        \n",
    "        return negsam\n",
    "    \n",
    "    def Get_pairs(self, window_size, n_negsam):\n",
    "        for c in corpus: # for each sentence          \n",
    "            for idx in range(len(c)): # for each idx\n",
    "                start = max(0,idx - window_size)\n",
    "                tail = min(idx + window_size, len(c)-1)\n",
    "            \n",
    "                #add negative samples to context\n",
    "                context = c[start:idx] + c[idx+1:tail+1]\n",
    "                needed_negsam = 2*window_size+n_negsam - len(context) # needed number of negative samples\n",
    "                negsam = self.Draw(needed_negsam) # draw negative samples\n",
    "            \n",
    "                #stack pairs        \n",
    "                \n",
    "                in_onehot = torch.zeros(self.vocab_size) #make input one-hot\n",
    "                in_onehot[word_dict[c[idx]]] = 1.0\n",
    "                \n",
    "                out = context+negsam\n",
    "                out_onehot = torch.zeros(len(out), self.vocab_size) #make output one-hot\n",
    "\n",
    "                for idx, txt in enumerate(out):\n",
    "                    out_onehot[idx,word_dict[txt]] = 1.0\n",
    "                \n",
    "                self.inputs.append(in_onehot)\n",
    "                self.targets.append(out_onehot)\n",
    "                self.labels.append([1]*len(context) + [0]*needed_negsam)\n",
    "                   \n",
    "        return self.inputs, self.targets, self.labels   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "id": "b838869e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self):\n",
    "        self.inputs = inputs\n",
    "        self.targets = targets\n",
    "        self.labels = torch.tensor(labels)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.inputs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        inputs = self.inputs[idx]\n",
    "        targets = self.targets[idx]\n",
    "        labels = self.labels[idx]\n",
    "                            \n",
    "        return inputs, targets, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "id": "4f9c9073",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.autograd import Variable\n",
    "import torch.optim as optim\n",
    "def train(inputs, targets, labels, emb_dim, lr, n_epoch, batch):\n",
    "    \n",
    "    vocab_size = len(vocab)\n",
    "    target_size = len(targets[0])\n",
    "    center_mat = Variable(torch.nn.init.xavier_normal_(torch.empty(emb_dim, vocab_size)).cuda(),requires_grad=True) # W mat\n",
    "    context_mat = Variable(torch.nn.init.xavier_normal_(torch.empty(vocab_size, emb_dim)).cuda(),requires_grad=True) # W' mat\n",
    "    \n",
    "    dataset = CustomDataset()\n",
    "    dataloader = DataLoader(dataset, batch_size=batch, shuffle=True)\n",
    "    \n",
    "    for epoch in range(n_epoch): \n",
    "        loss_value = 0\n",
    "        \n",
    "        \n",
    "        for batch_idx, samples in enumerate(dataloader):\n",
    "            I,T,L = samples\n",
    "        \n",
    "            x = Variable(I.cuda()) # word index of input\n",
    "            target = Variable(T.cuda()) # word index of target\n",
    "            h = torch.matmul(center_mat,x.T) # look-up table\n",
    "\n",
    "            tar = torch.matmul(target,context_mat)\n",
    "            z = torch.matmul(tar,h.T.unsqueeze(2))             \n",
    "            y = torch.LongTensor(L).cuda()\n",
    "\n",
    "            y_hat = F.log_softmax(z, dim=0).squeeze()\n",
    "            \n",
    "            loss = 0\n",
    "            for i in range(batch):\n",
    "                loss_tmp = F.nll_loss(y_hat[i], y[i])\n",
    "                loss+=loss_tmp\n",
    "            \n",
    "            loss_value += loss.item()\n",
    "            loss.backward()\n",
    "\n",
    "            center_mat.data -= lr * center_mat.grad.data # update center_mat\n",
    "            context_mat.data -= lr * context_mat.grad.data # update context_mat with context_tmp\n",
    "\n",
    "            # initialize gradient after update\n",
    "            center_mat.grad.data.zero_() \n",
    "            context_mat.grad.data.zero_()\n",
    "\n",
    "        if epoch % 100 == 0:\n",
    "            print(f\"Loss at this epoch {epoch+100}: {loss_value / vocab_size}\")\n",
    "        \n",
    "    return center_mat         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "id": "4f4484a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([20, 26, 30])\n",
      "torch.Size([30, 20])\n",
      "torch.Size([20, 26, 1])\n",
      "torch.Size([20, 26])\n",
      "torch.Size([20, 26])\n",
      "tensor([[1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0],\n",
      "        [1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0],\n",
      "        [1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0],\n",
      "        [1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0]], device='cuda:0')\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "0D or 1D target tensor expected, multi-target not supported",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32mC:\\Users\\IMEDIS~1\\AppData\\Local\\Temp/ipykernel_43100/3626163561.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     28\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 30\u001b[1;33m         \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnll_loss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_hat\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     31\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\NLP\\lib\\site-packages\\torch\\nn\\functional.py\u001b[0m in \u001b[0;36mnll_loss\u001b[1;34m(input, target, weight, size_average, ignore_index, reduce, reduction)\u001b[0m\n\u001b[0;32m   2530\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0msize_average\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mreduce\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2531\u001b[0m         \u001b[0mreduction\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlegacy_get_string\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msize_average\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreduce\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2532\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnll_loss_nd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_enum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreduction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2533\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2534\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: 0D or 1D target tensor expected, multi-target not supported"
     ]
    }
   ],
   "source": [
    "vocab_size = len(vocab)\n",
    "target_size = len(targets[0])\n",
    "center_mat = Variable(torch.nn.init.xavier_normal_(torch.empty(emb_dim, vocab_size)).cuda(),requires_grad=True) # W mat\n",
    "context_mat = Variable(torch.nn.init.xavier_normal_(torch.empty(vocab_size, emb_dim)).cuda(),requires_grad=True) # W' mat\n",
    "\n",
    "dataset = CustomDataset()\n",
    "dataloader = DataLoader(dataset, batch_size=20, shuffle=True)\n",
    "\n",
    "epoch = 0\n",
    "for batch_idx, samples in enumerate(dataloader):\n",
    "    I,T,L = samples\n",
    "    if batch_idx == 0:\n",
    "        x = Variable(I.cuda()) # word index of input\n",
    "        target = Variable(T.cuda()) # word index of target\n",
    "        h = torch.matmul(center_mat,x.T) # look-up table\n",
    "\n",
    "        tar = torch.matmul(target,context_mat)\n",
    "        z = torch.matmul(tar,h.T.unsqueeze(2))             \n",
    "        y = torch.LongTensor(L).cuda()\n",
    "\n",
    "        y_hat = F.log_softmax(z, dim=0).squeeze()\n",
    "        \n",
    "        \n",
    "        print(np.shape(tar))\n",
    "        print(np.shape(h))\n",
    "        print(np.shape(z))\n",
    "        print(np.shape(y_hat))\n",
    "        print(np.shape(y))\n",
    "        \n",
    "        loss = F.nll_loss(y_hat, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2961926a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from torch.autograd import Variable\n",
    "# import torch.optim as optim\n",
    "# def train(inputs, targets, labels, emb_dim, lr, n_epoch):\n",
    "    \n",
    "#     vocab_size = len(vocab)\n",
    "#     target_size = len(targets[0])\n",
    "#     center_mat = Variable(torch.nn.init.xavier_normal_(torch.empty(emb_dim, vocab_size)).cuda(),requires_grad=True) # W mat\n",
    "#     context_mat = Variable(torch.nn.init.xavier_normal_(torch.empty(vocab_size, emb_dim)).cuda(),requires_grad=True) # W' mat\n",
    "    \n",
    "#     for epoch in range(n_epoch): \n",
    "#         loss_value = 0\n",
    "#         for batch in range(len(inputs)):\n",
    "            \n",
    "#             x = Variable(inputs[batch].cuda()) # word index of input\n",
    "#             target = Variable(targets[batch].cuda()) # word index of target\n",
    "#             h = torch.matmul(center_mat,x) # look-up table\n",
    "            \n",
    "#             tar = torch.matmul(target,context_mat)\n",
    "#             z = torch.matmul(tar,h)             \n",
    "#             y = torch.LongTensor(labels[batch]).cuda()\n",
    "\n",
    "#             y_hat = F.log_softmax(z, dim=0)\n",
    "#             loss = F.nll_loss(y_hat, y)\n",
    "#             loss_value += loss.item()\n",
    "#             loss.backward()\n",
    "            \n",
    "#             center_mat.data -= lr * center_mat.grad.data # update center_mat\n",
    "#             context_mat.data -= lr * context_mat.grad.data # update context_mat with context_tmp\n",
    "            \n",
    "#             # initialize gradient after update\n",
    "#             center_mat.grad.data.zero_() \n",
    "#             context_mat.grad.data.zero_()\n",
    "        \n",
    "#         if epoch % 100 == 0:\n",
    "#             print(f\"Loss at this epoch {epoch+100}: {loss_value / vocab_size}\")\n",
    "        \n",
    "#     return center_mat         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "id": "96516666",
   "metadata": {},
   "outputs": [],
   "source": [
    "def TSNEplot(wordvec, vocab, perplexity):\n",
    "    \n",
    "    #matplotlib 패키지 한글 깨짐 처리\n",
    "    import matplotlib.pyplot as plt\n",
    "    %matplotlib notebook\n",
    "    plt.rc('font', family='Malgun Gothic') #윈도우, 구글 콜랩\n",
    "    plt.rcParams['axes.unicode_minus'] = False #한글 폰트 사용시 마이너스 폰트 깨짐 해결\n",
    "\n",
    "    from sklearn.manifold import TSNE\n",
    "    \n",
    "    wordvec_2d = TSNE(n_components=2, perplexity=perplexity, learning_rate='auto', init='random').fit_transform(wordvec)\n",
    "    plt.scatter(wordvec_2d[:,0], wordvec_2d[:,1], s=0.5)\n",
    "\n",
    "    test_words=[]\n",
    "    for i in range(len(vocab)):\n",
    "        test_words.append(index_dict[i])\n",
    "    \n",
    "    for i, word in enumerate(test_words):\n",
    "        plt.annotate(word, xy=(wordvec_2d[i, 0], wordvec_2d[i, 1]))\n",
    "    \n",
    "    return wordvec_2d    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "id": "228e9d9f",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index 10 is out of bounds for dimension 0 with size 10",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mC:\\Users\\IMEDIS~1\\AppData\\Local\\Temp/ipykernel_43100/212068408.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[1;31m# sentences, corpus, words, vocab, word_dict, index_dict = Prep().Get_clean(text) # 전처리\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[1;31m# inputs, targets, labels = Make_pairs().Get_pairs(window_size, n_negsam) # Input / target / label 생성\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 24\u001b[1;33m \u001b[0mcenter_mat\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0memb_dim\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_epoch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m20\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# Training\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     25\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[1;31m# lembedded = center_mat.to('cpu').detach().numpy()\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\IMEDIS~1\\AppData\\Local\\Temp/ipykernel_43100/3221566413.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(inputs, targets, labels, emb_dim, lr, n_epoch, batch)\u001b[0m\n\u001b[0;32m     30\u001b[0m             \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 32\u001b[1;33m                 \u001b[0mloss_tmp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnll_loss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_hat\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     33\u001b[0m                 \u001b[0mloss\u001b[0m\u001b[1;33m+=\u001b[0m\u001b[0mloss_tmp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     34\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: index 10 is out of bounds for dimension 0 with size 10"
     ]
    }
   ],
   "source": [
    "# Library import\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "all_text = Corpus.train.texts\n",
    "\n",
    "text = []\n",
    "for i in range(10):              # 게시글 10개만 추출\n",
    "    text.append(all_text[i])\n",
    "\n",
    "#parameters\n",
    "emb_dim = 30\n",
    "window_size = 3\n",
    "n_negsam = 20\n",
    "lr = 0.01\n",
    "n_epoch = 500\n",
    "\n",
    "# sentences, corpus, words, vocab, word_dict, index_dict = Prep().Get_clean(text) # 전처리\n",
    "# inputs, targets, labels = Make_pairs().Get_pairs(window_size, n_negsam) # Input / target / label 생성\n",
    "center_mat = train(inputs, targets, labels, emb_dim, lr, n_epoch, batch=20) # Training\n",
    "\n",
    "# lembedded = center_mat.to('cpu').detach().numpy()\n",
    "# wordvec_2d = TSNEplot(embedded.T, vocab, perplexity=4) # TNSE plotting"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
