{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ce82658",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f02906d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import kss\n",
    "import re\n",
    "from konlpy.tag import Mecab\n",
    "\n",
    "class Prep():\n",
    "        \n",
    "    def Clean_text(self, text): # noisy한 문장을 cleaning\n",
    "        self.sentences = []\n",
    "        \n",
    "        for i in range(len(text)):\n",
    "            if (i+1) % 10 == 0:\n",
    "                print(str(i+1) + 'th sentence is spliting...')\n",
    "        \n",
    "            temp = re.sub(\"[^가-힣a-z0-9.?]\", ' ', text[i]) #한글, 영어, 숫자, 온점, 물음표가 아닌 것을 공백으로 삭제\n",
    "            temp = re.sub(\"[.]{2,}\", \".\", temp) # 반복되는 온점 (...) 삭제\n",
    "            temp = re.sub(\"[?]{2,}\", \"?\", temp) # 반복되는 물음표 (?) 삭제\n",
    "            temp = re.sub(\"[!]{2,}\", \"!\", temp) # 반복되는 느낌표 (!) 삭제\n",
    "            temp = re.sub(\"[' ']{2,}\", \" \", temp) # 반복되는 공백 삭제 \n",
    "            temp = kss.split_sentences(temp)  #문장 분리\n",
    "\n",
    "            for tmp in temp:\n",
    "                self.sentences.append(tmp)\n",
    "        \n",
    "        return self.sentences\n",
    "\n",
    "        \n",
    "    def Tokenizer(self, sentences): # cleaned data에 대한 형태소 분석 후 vocabulary 구축\n",
    "        \n",
    "        self.corpus = []\n",
    "        self.words = []\n",
    "        self.vocab = []\n",
    "        \n",
    "        tokenizer = Mecab(dicpath=r\"C:\\mecab\\mecab-ko-dic\")\n",
    "        \n",
    "        for sent in self.sentences:\n",
    "            temp = tokenizer.morphs(sent)\n",
    "            self.corpus.append(temp)\n",
    "            \n",
    "            for tmp in temp:\n",
    "                self.words.append(tmp)\n",
    "\n",
    "        self.vocab = set(self.words)\n",
    "        \n",
    "        return self.corpus, self.words, self.vocab\n",
    "    \n",
    "    def Make_dict(self,vocab): # 만들어진 Vocabulary를 기준으로 단어-정수 및 정수-단어 인덱싱 dictionary 생성\n",
    "        self.word_dict = {w: i for i, w in enumerate(vocab)}\n",
    "        self.index_dict = {i: w for i, w in enumerate(vocab)}\n",
    "        return self.word_dict, self.index_dict \n",
    "    \n",
    "    def Get_clean(self, text): #위의 세가지 메소드를 한번에 진행                \n",
    "        sentences =  self.Clean_text(text)\n",
    "        corpus, words, vocab = self.Tokenizer(sentences)\n",
    "        word_dict, index_dict = self.Make_dict(vocab)\n",
    "        \n",
    "        return sentences, corpus, words, vocab, word_dict, index_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f89a062",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Make_pairs(): # Input과 target, label을 만들어주는 클래스\n",
    "    \n",
    "    def __init__(self):\n",
    "        \n",
    "        from collections import defaultdict\n",
    "        self.inputs = [] \n",
    "        self.targets = []\n",
    "        self.labels = []\n",
    "        \n",
    "        # Negative sampling 정의\n",
    "        self.wordFreq = defaultdict(int)\n",
    "        for word in words:\n",
    "            self.wordFreq[word] += 1\n",
    "        \n",
    "        self.SumFreq = sum([freq**(3/4) for freq in self.wordFreq.values()]) # 분모\n",
    "        self.wordProb = {word:((freq**(3/4))/self.SumFreq) for word, freq in self.wordFreq.items()} #샘플링 확률분포\n",
    "        \n",
    "    def Draw(self,n_sample): # n_sample : sample을 몇 개를 뽑을건지?\n",
    "        negsam = []\n",
    "        for i in range(n_sample):\n",
    "            negsam.append(np.random.choice(list(self.wordProb.keys()), p=list(self.wordProb.values()))) #확률분포 P에 따른 추출\n",
    "        \n",
    "        return negsam\n",
    "    \n",
    "    def Get_pairs(self, window_size, n_negsam):\n",
    "        for c in corpus: # for each sentence          \n",
    "            for idx in range(len(c)): # for each idx\n",
    "                start = max(0,idx - window_size)\n",
    "                tail = min(idx + window_size, len(c)-1)\n",
    "            \n",
    "                #add negative samples to context\n",
    "                context = c[start:idx] + c[idx+1:tail+1]\n",
    "                needed_negsam = 2*window_size+n_negsam - len(context) # needed number of negative samples\n",
    "                negsam = self.Draw(needed_negsam) # draw negative samples\n",
    "            \n",
    "                #stack pairs\n",
    "                self.inputs.append(c[idx])\n",
    "                self.targets.append(context+negsam)\n",
    "                self.labels.append([1]*len(context) + [0]*needed_negsam)\n",
    "                  \n",
    "        return self.inputs, self.targets, self.labels   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1e10f20",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.autograd import Variable\n",
    "def train(inputs, targets, labels, emb_dim, lr, n_epoch):\n",
    "    \n",
    "    vocab_size = len(vocab)\n",
    "    target_size = len(targets[0])\n",
    "    center_mat = Variable(torch.nn.init.xavier_normal_(torch.empty(emb_dim, vocab_size)),requires_grad=True).float() # W mat\n",
    "    context_mat = Variable(torch.nn.init.xavier_normal_(torch.empty(vocab_size, emb_dim)),requires_grad=True).float() # W' mat\n",
    "    \n",
    "    for epoch in range(n_epoch): \n",
    "        loss_value = 0\n",
    "        for batch in range(len(inputs)): \n",
    "            x = word_dict[inputs[batch]] # word index of input\n",
    "            h = center_mat[:,x] # look-up table\n",
    "            \n",
    "            tar_ind = [word_dict[tar] for tar in targets[batch]]  # make target index\n",
    "                \n",
    "            context_tmp = Variable(context_mat[tar_ind,:], requires_grad=True).float() # embedded target vectors\n",
    "            z = torch.matmul(context_tmp,h) \n",
    "            y = torch.LongTensor(labels[batch])\n",
    "\n",
    "            y_hat = F.log_softmax(z, dim=0)\n",
    "            loss = F.nll_loss(y_hat, y)\n",
    "            loss_value += loss.item()\n",
    "            loss.backward()\n",
    "            \n",
    "            center_mat.data -= lr * center_mat.grad.data # update center_mat\n",
    "            context_mat.data[tar_ind,:] -= lr * context_tmp.grad.data # update context_mat with context_tmp\n",
    "            \n",
    "            # initialize gradient after update\n",
    "            center_mat.grad.data.zero_() \n",
    "            context_tmp.grad.data.zero_()\n",
    "        \n",
    "        if epoch % 100 == 0:\n",
    "            print(f\"Loss at this epoch {epoch+100}: {loss_value / vocab_size}\")\n",
    "        \n",
    "    return center_mat         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f546f7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def TSNEplot(wordvec, vocab, perplexity):\n",
    "    \n",
    "    #matplotlib 패키지 한글 깨짐 처리\n",
    "    import matplotlib.pyplot as plt\n",
    "    %matplotlib notebook\n",
    "    plt.rc('font', family='Malgun Gothic') #윈도우, 구글 콜랩\n",
    "    plt.rcParams['axes.unicode_minus'] = False #한글 폰트 사용시 마이너스 폰트 깨짐 해결\n",
    "\n",
    "    from sklearn.manifold import TSNE\n",
    "    \n",
    "    wordvec_2d = TSNE(n_components=2, perplexity=perplexity, learning_rate='auto', init='random').fit_transform(wordvec)\n",
    "    plt.scatter(wordvec_2d[:,0], wordvec_2d[:,1], s=0.5)\n",
    "\n",
    "    test_words=[]\n",
    "    for i in range(len(vocab)):\n",
    "        test_words.append(index_dict[i])\n",
    "    \n",
    "    for i, word in enumerate(test_words):\n",
    "        plt.annotate(word, xy=(wordvec_2d[i, 0], wordvec_2d[i, 1]))\n",
    "    \n",
    "    return wordvec_2d    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a13ddef5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Library import\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# 간단한 Corpus\n",
    "text = ['대한민국의 수도는 서울',\n",
    "       '독일의 수도는 베를린',\n",
    "       '프랑스의 수도는 파리',\n",
    "       '스위스의 수도는 베른',\n",
    "       '일본의 수도는 도쿄',\n",
    "       '이탈리아의 수도는 로마',\n",
    "       '영국의 수도는 런던',\n",
    "       '이집트의 수도는 카이로'\n",
    "       ]        \n",
    "\n",
    "#parameters\n",
    "emb_dim = 20\n",
    "window_size = 2\n",
    "n_negsam = 3\n",
    "lr = 0.01\n",
    "n_epoch = 1000\n",
    "\n",
    "sentences, corpus, words, vocab, word_dict, index_dict = Prep().Get_clean(text) # 전처리\n",
    "inputs, targets, labels = Make_pairs().Get_pairs(window_size, n_negsam) # Input / target / label 생성\n",
    "center_mat = train(inputs, targets, labels, emb_dim, lr, n_epoch) # Training\n",
    "wordvec_2d = TSNEplot(center_mat.data.T, vocab, perplexity=5) # TNSE plotting"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CUDA",
   "language": "python",
   "name": "cuda"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
