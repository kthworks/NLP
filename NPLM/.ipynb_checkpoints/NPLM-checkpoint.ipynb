{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "83be570d",
   "metadata": {},
   "source": [
    "## Neural Probabilistic Language Model (NPLM) 구현"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fa765d9",
   "metadata": {},
   "source": [
    "이번 포스팅에서는 저번 시간에 공부한 [NPLM](https://www.jmlr.org/papers/volume3/bengio03a/bengio03a.pdf) 모델을 직접 구현해보도록 하겠습니다.\n",
    "\n",
    "일반적으로 구현을 할 때는 전체적인 흐름을 미리 생각하고 구현을 하기 때문에 함수나 모델에 대한 정의를 미리 하는데, 저는 그렇게 했을 때 이해하기가 조금 더 어려움을 느꼈습니다. \n",
    "\n",
    "그래서, 여러분들이 글을 읽고 따라오시면서 최대한 쉬운 이해를 하실 수 있도록 의식의 흐름대로 구현을 한 후에, 마지막에 전체적으로 코드를 정리하도록 하겠습니다.\n",
    "\n",
    "먼저, 기본 라이브러리들을 import 하겠습니다.\n",
    "pytorch와 neural network, 그리고 학습하면서 weight update를 하기 위한 optimizer를 가져옵니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f006f4d1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56242b21",
   "metadata": {},
   "source": [
    "저는 모델을 만들기 전에 **우선 '데이터'에 대한 이해와 준비가 필요**하다고 생각합니다.\n",
    "데이터가 정확히 어떤 형식으로 준비되어야 하는지 알지 못한다면 모델이나 함수를 구현할때 더욱 헷갈리는 것 같습니다.\n",
    "\n",
    "직관적인 이해를 위해 오픈 데이터셋을 사용하지 않고, 간단한 몇 개의 문장으로 데이터를 준비해 보겠습니다.\n",
    "\n",
    "우선, NPLM은 [n-gram](https://en.wikipedia.org/wiki/N-gram)을 기반으로 하기 때문에 n을 정해 보겠습니다. 대부분 n-gram에서 n=3으로 하는 tri-gram을 많이 사용하기 때문에, 3개의 어절로 구성된 문장들을 준비했습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d8a3411b",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = ['코딩은 시작이 반이다', '나는 오늘도 주짓수', '코딩은 어려워 짜증나', 'NLP 니가 뭔데', '내가 바로 공주다']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b7577a3",
   "metadata": {},
   "source": [
    "위에서 준비한 이 문장들이 우리가 확보한 dataset, 즉 **corpus(말뭉치)**가 됩니다.\n",
    "\n",
    "다음으로, 문장으로 구성된 데이터 셋을 단어 단위로 쪼개는 **Tokenization(토큰화)** 과정이 필요합니다. \n",
    "\n",
    "이번 포스팅에서 준비한 corpus는 띄어쓰기를 기준으로 아주 간단하게 Tokenization을 진행할 수 있지만, 사실 실제 대용량 데이터를 다룰 때에는 중복 단어, 특수 문자, 띄어쓰기, 불필요한 단어, 오타 등을 처리하기 위해 다양한 전처리 과정들을 반드시 진행해야 합니다.\n",
    "\n",
    "전처리까지 다루기에는 너무 복잡해지므로 본 포스팅에서는 띄어쓰기를 기준으로 간단히 단어를 나누고, 중복되는 단어만 제거하는 방식으로 토큰화를 진행하겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "db364611",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = \" \".join(corpus).split()  # \" \"(띄어쓰기)를 기준으로 corpus 안의 데이터를 split(분리)해줍니다.\n",
    "tokens = list(set(tokens)) # set 함수는 중복 없이 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b939f696",
   "metadata": {},
   "source": [
    "위와 같이 띄어쓰기를 기준으로 단어들을 구분한후, 단어들을 모두 모았습니다.\n",
    "corpus가 준비되었으니, 각 단어의 인덱스를 만들어 주겠습니다.\n",
    "인덱싱은 dictionary를 활용하여 단어를 알 때 인덱스를 찾거나, 인덱스를 알 때 단어를 찾을 수 있도록 두가지를 만들어 줍니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dd146a62",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_dict = {w: i for i, w in enumerate(tokens)} # word -> index\n",
    "index_dict = {i: w for i, w in enumerate(tokens)} # index -> word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "46ec0cfc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'어려워': 0,\n",
       " '짜증나': 1,\n",
       " '바로': 2,\n",
       " '나는': 3,\n",
       " '주짓수': 4,\n",
       " 'NLP': 5,\n",
       " '내가': 6,\n",
       " '코딩은': 7,\n",
       " '반이다': 8,\n",
       " '니가': 9,\n",
       " '공주다': 10,\n",
       " '오늘도': 11,\n",
       " '시작이': 12,\n",
       " '뭔데': 13}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cff9648a",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: '어려워',\n",
       " 1: '짜증나',\n",
       " 2: '바로',\n",
       " 3: '나는',\n",
       " 4: '주짓수',\n",
       " 5: 'NLP',\n",
       " 6: '내가',\n",
       " 7: '코딩은',\n",
       " 8: '반이다',\n",
       " 9: '니가',\n",
       " 10: '공주다',\n",
       " 11: '오늘도',\n",
       " 12: '시작이',\n",
       " 13: '뭔데'}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f156609",
   "metadata": {},
   "source": [
    "다음으로, input과 target을 나눠주겠습니다. 우리의 목적은 단어 2개를 참고하여 그 다음에 올 단어를 예측하는 것입니다(tri-gram).\n",
    "각 단어들을 추출한 후에, 추후 Embedding을 진행하기 위해 미리 word_dict를 이용하여 인덱스로 변환해주도록 하겠습니다.\n",
    "마지막으로, 모델에 들어가는 input 형태는 tensor 타입으로 들어가기 때문에 최종 출력을 tensor타입으로 변환해 줍니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cfe6cbad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_input_target():\n",
    "    input_batch = []\n",
    "    target_batch = []\n",
    "    \n",
    "    for sentence in corpus:\n",
    "        word = sentence.split()  # 띄어쓰기 기준으로 단어 구분\n",
    "        input = [word_dict[n] for n in word[:-1]] # 1 ~ n-1 번째 단어를 input으로 사용. n=3이므로 첫 두 단어 사용\n",
    "        target = word_dict[word[-1]] # n번째 단어를 target으로 사용\n",
    "        \n",
    "        input_batch.append(input)\n",
    "        target_batch.append(target)\n",
    "        \n",
    "    return torch.LongTensor(input_batch), torch.LongTensor(target_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d83ecdc3",
   "metadata": {},
   "source": [
    "input과 target이 인덱스를 기준으로 잘 정리 되었는지 확인해보겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "320f6ee3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input :  tensor([[ 7, 12],\n",
      "        [ 3, 11],\n",
      "        [ 7,  0],\n",
      "        [ 5,  9],\n",
      "        [ 6,  2]])\n",
      "Target :  tensor([ 8,  4,  1, 13, 10])\n"
     ]
    }
   ],
   "source": [
    "input_batch, target_batch = make_input_target()\n",
    "print('Input : ', input_batch)\n",
    "print('Target : ', target_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41c13a85",
   "metadata": {},
   "source": [
    "임베딩 과정부터는 모델 안에서 다루어지는데요,\n",
    "먼저 모델에 사용되는 각 요소들에 대해서 한번 더 짚고 넘어가겠습니다.\n",
    "<center><img src=\"./img/model.png\" width=\"700\" height=\"700\"></center>\n",
    "\n",
    "target을 예측하기 위한 output 수식은 $Y_{w_{t}} = b + Wx + U( tanh(Hx_{t}+d) )$ 였습니다.      \n",
    "\n",
    "V : 총 토큰(단어) 개수   \n",
    "m : 임베딩 벡터 차원 (단어를 몇 차원의 벡터로 임베딩할건지 결정. 예를 들어 m=2이면 2차원의 벡터가 생성됨)   \n",
    "n_hidden : hidden layer의 unit 수   \n",
    "\n",
    "\n",
    "C (Embedding layer) : 각 인덱스에 있는 단어가 embedding되어 vector 형태로 들어 있음. 차원 = V x m    \n",
    "H : Embedding layer와 hidden layer 사이의 weight. 차원 = ( (n-1) * m ) x n_hidden   \n",
    "d : hidden layer의 bias. 차원 = n_hidden       \n",
    "U : hidden layer가 activation function (tanh)까지 거친 후와 output 사이의 weight. 차원 = n_hidden x V   \n",
    "W : Embedding layer에서 output까지 직접 연결하기 위한 weight. 차원 = ( (n-1) * m ) x V   \n",
    "b : 가장 마지막에(out 직전에) 더해주는 bias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f4cc13a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NPLM(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NPLM, self).__init__()\n",
    "        self.C = nn.Embedding(V, m)\n",
    "        self.H = nn.Linear( (n-1)*m, n_hidden, bias=False)\n",
    "        self.d = nn.Parameter(torch.ones(n_hidden))\n",
    "        self.U = nn.Linear( n_hidden, V, bias=False)\n",
    "        self.W = nn.Linear( (n-1)*m, V, bias=False)\n",
    "        self.b = nn.Parameter(torch.ones(V))\n",
    "    \n",
    "    def forward(self, X):\n",
    "        X = self.C(X) # 인덱스 -> embedding vector\n",
    "        X = X.view(-1, (n-1)*m) # (n-1)개의 embedding vector를 unroll 하여 [batch_size x (n-1)*m] 으로 만들어줌\n",
    "        tanh = torch.tanh(self.d + self.H(X)) # [batch_size, n_hidden]\n",
    "        output = self.b + self.W(X) + self.U(tanh) # [batch_size, V]\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55b3f0c2",
   "metadata": {},
   "source": [
    "자, 이렇게 모델도 구축을 완료했습니다!\n",
    "이제, 모든 요소들을 포함한 메인 루프를 완성해 봅시다 : )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a7dde5b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1000 cost = 0.063062\n",
      "Epoch: 2000 cost = 0.013515\n",
      "Epoch: 3000 cost = 0.005067\n",
      "Epoch: 4000 cost = 0.002322\n",
      "Epoch: 5000 cost = 0.001171\n",
      "Epoch: 6000 cost = 0.000622\n",
      "Epoch: 7000 cost = 0.000341\n",
      "Epoch: 8000 cost = 0.000190\n",
      "Epoch: 9000 cost = 0.000107\n",
      "Epoch: 10000 cost = 0.000061\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__': #코드를 직접 실행했을때만 작동하고, import 될때는 작동하지 않게 만들어줌\n",
    "    n = 3\n",
    "    n_hidden = 2\n",
    "    m = 2\n",
    "    V = len(tokens)\n",
    "    \n",
    "    model = NPLM()\n",
    "    \n",
    "    criterion = nn.CrossEntropyLoss() # loss function으로 cross entropy 사용\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001) # optimizer로 adam 사용\n",
    "    \n",
    "    input_batch, target_batch = make_input_target()\n",
    "    \n",
    "    # Training\n",
    "    for epoch in range(10000):\n",
    "        optimizer.zero_grad() # optimizer 초기화\n",
    "        output = model(input_batch) # output 계산\n",
    "        \n",
    "        loss = criterion(output, target_batch)     # loss 계산\n",
    "    \n",
    "        if (epoch+1) % 1000 == 0:\n",
    "            print('Epoch:', '%04d' % (epoch+1), 'cost =', '{:.6f}'.format(loss))   #1000번째 epoch마다 loss 출력\n",
    "        \n",
    "        loss.backward() # backpropagation\n",
    "        optimizer.step() # weight update"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a28aecd2",
   "metadata": {},
   "source": [
    "모델 학습이 잘 된 것 같습니다!   \n",
    "그럼 이제 예측 결과를 살펴볼까요?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "46af7adb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['코딩은', '시작이']  ->  반이다\n",
      "['나는', '오늘도']  ->  주짓수\n",
      "['코딩은', '어려워']  ->  짜증나\n",
      "['NLP', '니가']  ->  뭔데\n",
      "['내가', '바로']  ->  공주다\n"
     ]
    }
   ],
   "source": [
    "# Prediction\n",
    "predict = model(input_batch).data.max(1, keepdim=True)[1]\n",
    "\n",
    "for i in corpus:\n",
    "    print(i.split()[:n-1], ' -> ', i.split()[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d740dc3a",
   "metadata": {},
   "source": [
    "앞의 두 단어를 이용해서 뒤의 단어를 잘 예측하는 것을 확인할 수 있습니다.\n",
    "\n",
    "또 하나, 재밌는 것을 확인해보고자 합니다.\n",
    "학습된 embedding vector들을 2차원 평면상에 뿌려보았을때, 서로 관련있는 단어들끼리 뭉쳐질까요?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8d3f6494",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "plt.rc('font', family='Malgun Gothic') # 한글 출력을 가능하게 만들기\n",
    "plt.rc('axes', unicode_minus=False)   # 한글 출력을 가능하게 만들기\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.scatter(model.C.weight[:,0].tolist(), model.C.weight[:,1].tolist())\n",
    "\n",
    "for i, txt in enumerate(tokens):\n",
    "    ax.annotate(txt, (model.C.weight[i,0].tolist(), model.C.weight[i,1].tolist()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b15f812a",
   "metadata": {},
   "source": [
    "평면상에 표상은 되었다만 딱히 큰 연관성은 보이지 않는 것 같습니다. 풍부한 데이터를 이용하여 학습시킨다면 품사 간의 관계나 단어의 유의성 등을 훨씬 잘 표상하지 않을까 싶습니다.\n",
    "\n",
    "마지막으로, 전체 코드 정리본을 올리고 마무리하겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1e163f6f",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torch'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_3764/2162331030.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnn\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnn\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptim\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0moptim\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mmatplotlib\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpyplot\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'torch'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim \n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "def make_input_target():\n",
    "    input_batch = []\n",
    "    target_batch = []\n",
    "    \n",
    "    for sentence in corpus:\n",
    "        word = sentence.split()  # 띄어쓰기 기준으로 단어 구분\n",
    "        input = [word_dict[n] for n in word[:-1]] # 1 ~ n-1 번째 단어를 input으로 사용. n=3이므로 첫 두 단어 사용\n",
    "        target = word_dict[word[-1]] # n번째 단어를 target으로 사용\n",
    "        \n",
    "        input_batch.append(input)\n",
    "        target_batch.append(target)\n",
    "        \n",
    "    return torch.LongTensor(input_batch), torch.LongTensor(target_batch)\n",
    "\n",
    "class NPLM(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NPLM, self).__init__()\n",
    "        self.C = nn.Embedding(V, m)\n",
    "        self.H = nn.Linear( (n-1)*m, n_hidden, bias=False)\n",
    "        self.d = nn.Parameter(torch.ones(n_hidden))\n",
    "        self.U = nn.Linear( n_hidden, V, bias=False)\n",
    "        self.W = nn.Linear( (n-1)*m, V, bias=False)\n",
    "        self.b = nn.Parameter(torch.ones(V))\n",
    "    \n",
    "    def forward(self, X):\n",
    "        X = self.C(X) # 인덱스 -> embedding vector\n",
    "        X = X.view(-1, (n-1)*m) # (n-1)개의 embedding vector를 unroll 하여 [batch_size x (n-1)*m] 으로 만들어줌\n",
    "        tanh = torch.tanh(self.d + self.H(X)) # [batch_size, n_hidden]\n",
    "        output = self.b + self.W(X) + self.U(tanh) # [batch_size, V]\n",
    "        return output\n",
    "    \n",
    "if __name__ == '__main__': #코드를 직접 실행했을때만 작동하고, import 될때는 작동하지 않게 만들어줌\n",
    "    \n",
    "    corpus = ['코딩은 시작이 반이다', '나는 오늘도 주짓수', '코딩은 어려워 짜증나', 'NLP 니가 뭔데', '내가 바로 공주다']\n",
    "    tokens = \" \".join(corpus).split()  # \" \"(띄어쓰기)를 기준으로 corpus 안의 데이터를 split(분리)해줍니다.\n",
    "    tokens = list(set(tokens)) # set 함수는 중복 없이 \n",
    "    word_dict = {w: i for i, w in enumerate(tokens)} # word -> index\n",
    "    index_dict = {i: w for i, w in enumerate(tokens)} # index -> word\n",
    "    \n",
    "    n = 3\n",
    "    n_hidden = 2\n",
    "    m = 2\n",
    "    V = len(tokens)\n",
    "    \n",
    "    model = NPLM()\n",
    "    \n",
    "    criterion = nn.CrossEntropyLoss() # loss function으로 cross entropy 사용\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001) # optimizer로 adam 사용\n",
    "    \n",
    "    input_batch, target_batch = make_input_target()\n",
    "    \n",
    "    # Training\n",
    "    for epoch in range(10000):\n",
    "        optimizer.zero_grad() # optimizer 초기화\n",
    "        output = model(input_batch) # output 계산\n",
    "        \n",
    "        loss = criterion(output, target_batch)     # loss 계산\n",
    "    \n",
    "        if (epoch+1) % 1000 == 0:\n",
    "            print('Epoch:', '%04d' % (epoch+1), 'cost =', '{:.6f}'.format(loss))   #1000번째 epoch마다 loss 출력\n",
    "        \n",
    "        loss.backward() # backpropagation\n",
    "        optimizer.step() # weight update\n",
    "\n",
    "# Prediction\n",
    "predict = model(input_batch).data.max(1, keepdim=True)[1]\n",
    "\n",
    "for i in corpus:\n",
    "    print(i.split()[:n-1], ' -> ', i.split()[-1])\n",
    "    \n",
    "# Scatter embedded vectors\n",
    "plt.rc('font', family='Malgun Gothic') # 한글 출력을 가능하게 만들기\n",
    "plt.rc('axes', unicode_minus=False)   # 한글 출력을 가능하게 만들기\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.scatter(model.C.weight[:,0].tolist(), model.C.weight[:,1].tolist())\n",
    "\n",
    "for i, txt in enumerate(tokens):\n",
    "    ax.annotate(txt, (model.C.weight[i,0].tolist(), model.C.weight[i,1].tolist()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "993899a0",
   "metadata": {},
   "source": [
    "### Reference\n",
    "\n",
    "https://soobarkbar.tistory.com/8   \n",
    "https://ratsgo.github.io/from%20frequency%20to%20semantics/2017/03/29/NNLM/   \n",
    "https://github.com/graykode/nlp-tutorial/blob/master/1-1.NNLM/NNLM.py   \n",
    "\n",
    "NPLM paper : https://www.jmlr.org/papers/volume3/bengio03a/bengio03a.pdf   "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
