{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8b288d10",
   "metadata": {},
   "source": [
    "# Neural Probabilistic Language Model (NPLM)\n",
    "\n",
    "안녕하세요, 이번 포스팅에서는 [Neural Probabilistic Language Model(LPLM)](https://www.jmlr.org/papers/volume3/bengio03a/bengio03a.pdf)에 대해서 공부하고자 합니다.\n",
    "NPLM은 2003년에 AI 4대천왕 중 한분인 Yoshua Bengio 교수님께서 제안하셨고, 당시 많은 주목을 받았습니다.    \n",
    "\n",
    "### 논문 발표 당시 (2003)의 최신 기법은..?\n",
    "\n",
    "2003년 당시의 최신 기법은 통계적 언어 모델 (Statistical Language Model, SLP) 하나인 [n-gram](https://en.wikipedia.org/wiki/N-gram) 기법이었습니다. n 개의 단어들 중 앞의 첫번째부터 n-1째까지의 단어가 주어졌을 때 마지막 n번째를 학습한 데이터의 통계 기반으로 예측하는 기법입니다.  \n",
    "\n",
    "예를 들어 **'가는 말이 고와야 오는 말도 ___'** 라는 문장이 있다고 합시다. 빈칸에 들어갈 말은 '곱다'가 되겠죠. n=3인 tri-gram 모델에서는 '곱다' 라는 단어를 예측하기 위해 '오는', '말도' 2개의 단어만을 참고합니다. 이를 수식으로 일반화하여 나타내면 아래와 같습니다.  \n",
    "  \n",
    "  \n",
    "$$ P( w_{t} | w_{t-(n-1)},...,w_{t-1}) = \\frac{exp(y_{w_{t}})}{\\Sigma_{i}exp(y_{i})}$$  \n",
    "  \n",
    "  \n",
    "t번째 단어를 예측하기 위해서는, t번째 단어 앞에 있는 n-1개의 단어들이 주어졌을 때 가장 다음으로 올 법한 단어를 통계적으로 찾는 것입니다. 이 조건부 확률을 최대화 하는방향으로 학습하게 되면, '오는', '말도' 두 단어가 주어졌을 때, 수많은 단어들 중 '곱다'에 대해서 가장 높은 확률값을 나타내게 되는 것입니다.\n",
    "\n",
    "\n",
    "### N-gram의 단점과 NPLM에서 제안한 개선 방법\n",
    "\n",
    "n-gram모델은 당시 최신 기법이었지만 **몇 가지 단점**들이 있었습니다.  \n",
    "  \n",
    "   \n",
    "1. 참고한 n-1개의 단어가 **Corpus에 존재하지 않을 때 조건부 확률이 0**이 되는 점\n",
    "2. 차원의 저주 (Curse of dimensionality) : n을 크게 설정할수록 **단어가 corpus에 존재하지 않을 확률이 증가**하고, **데이터가 sparse**해지는 점\n",
    "3. **단어 및 문장 간 유사도를 계산할 수 없다**는 점.\n",
    "\n",
    "기존의 n-gram은 컴퓨터에게 단어를 인식시키기 위해 하나의 요소만 1이고 나머지는 0인 vector로 변환하는 [one-hot encoding](https://en.wikipedia.org/wiki/One-hot) 방법을 사용했습니다.  \n",
    "  예를 들어서 '빨간', '사과는', '맛있다' 이렇게 3가지 단어에 one-hot encoding을 하면 아래와 같이 변환됩니다.\n",
    "\n",
    "'빨간  ' = [0 0 1]  \n",
    "'사과는' = [0 1 0]  \n",
    "'맛있다' = [1 0 0]\n",
    "\n",
    "\n",
    "겉보기엔 아주 간결하고 쉽게 표현이 되는 것 같지만, 만약 단어의 수가 아주 많아진다면 어떻게 될까요?  \n",
    "변환된 데이터는 차원의 수는 아주 크지만 거의 대부분이 0인 **sparse vector**가 됩니다.  \n",
    "이렇게 되면 메모리(저장공간)도 부족하고 계산 복잡도도 걷잡을 수 없이 늘어나게 되겠죠.(위에서 언급한 2번째 단점)\n",
    "\n",
    "또 다른 단점이 있습니다. One-hot vector들은 자신의 고유한 차원에서만 1의 값을 가지고 나머지 차원에서는 모두 0을 가지므로 서로 내적(inner product)를 하면 항상 0이 나오는데요, 두 벡터간의 내적이 0이면 두 벡터는 서로 직교(orthogonal)합니다. 즉, **one-hot vector는 서로 직교(orthogonal)하므로 각각 독립**이 됩니다. 실제로 단어들 간에는 여러가지 연관성(품사, 유의어, 동의어 등)을 가질 수 있지만, **one-hot vector로 표현하면 모든 단어를 서로 독립으로 간주하기 때문에 이러한 단어 간의 연관성을 담아내지 못합니다.**\n",
    "\n",
    "NPLM에서는 이러한 문제점들을 해결하기 위해 **분산 표현 (Distributed Representation)**이라는 개념을 제안합니다.\n",
    "\n",
    "**밀집벡터(Dense vector)**로 임베딩(Embedding)하여 차원의 저주 (Curse of dimensionality)를 해결하고, **단어의 유사도를 표현** 할 수 있으며 모든 n-gram을 저장하지 않아도 되기 때문에 기존 n-gram 언어 모델보다 **저장 공간의 이점**을 가집니다.    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ea25934",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
